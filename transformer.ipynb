{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karam-koujan/Transformer/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "0Ry67Bu3sUZ2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def get_device():\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = get_device()\n",
        "class Positional_embedding(nn.Module):\n",
        "      def __init__(self,d_model,max_len) :\n",
        "            super(Positional_embedding,self).__init__()\n",
        "            self.d_model = d_model\n",
        "            self.max_len = max_len\n",
        "            self.positional_embedding = torch.zeros((max_len,d_model)).to(device)\n",
        "            pos = torch.arange(0,max_len,dtype=torch.float).unsqueeze(1)\n",
        "            div = torch.pow(1000,torch.arange(0,d_model,2,dtype=torch.float)/d_model)\n",
        "            self.positional_embedding[:,0::2] = torch.sin(pos/div)\n",
        "            self.positional_embedding[:,1::2] = torch.cos(pos/div)\n",
        "      def forward(self,x) :\n",
        "            print(\"============ positional embedding ==========\",x.size(),self.positional_embedding.size())\n",
        "            x = x +  self.positional_embedding\n",
        "            return x\n",
        "class Tokenizer(nn.Module) :\n",
        "      def __init__(self,model_d,max_sequence_length,language_to_index,start_token,end_token,pad_token,dropout_p=0.1) :\n",
        "            super(Tokenizer,self).__init__()\n",
        "            self.vocab_size = len(language_to_index) + 3\n",
        "            self.embedding = nn.Embedding(self.vocab_size,model_d)\n",
        "            self.max_sequence_length = max_sequence_length\n",
        "            self.language_to_index = language_to_index\n",
        "            self.start_token = start_token\n",
        "            self.end_token = end_token\n",
        "            self.pad_token = pad_token\n",
        "            self.dropout = nn.Dropout(p=dropout_p)\n",
        "            self.positional_embedding = Positional_embedding(model_d,max_sequence_length)\n",
        "      def batch_tokenization(self,batch,start_token,end_token) :\n",
        "              def sentence_tokenize(sentence,start_token,end_token) :\n",
        "                      sentence_to_index = []\n",
        "                      for tokenIdx,token in enumerate(list(sentence)) :\n",
        "                              if tokenIdx + int(start_token) + int(end_token) + 1 >= self.max_sequence_length :\n",
        "                                          break\n",
        "                              if token in self.language_to_index:\n",
        "                                      sentence_to_index.append(self.language_to_index[token] )\n",
        "                              else :\n",
        "                                      sentence_to_index.append(self.language_to_index['<unk>'])\n",
        "                      if start_token :\n",
        "                            sentence_to_index.insert(0,self.language_to_index[self.start_token])\n",
        "                      if end_token :\n",
        "                            sentence_to_index.append(self.language_to_index[self.end_token])\n",
        "\n",
        "                      for _ in range(len(sentence_to_index),self.max_sequence_length):\n",
        "                                  sentence_to_index.append(self.language_to_index[self.pad_token])\n",
        "                      return torch.tensor(sentence_to_index).to(device)\n",
        "\n",
        "              sentence_batch = []\n",
        "              for sentence_idx in range(len(batch)) :\n",
        "                    sentence_batch.append(sentence_tokenize(batch[sentence_idx],start_token,end_token))\n",
        "\n",
        "              sentence_batch = torch.stack(sentence_batch)\n",
        "              return sentence_batch.to(device)\n",
        "      def forward(self,x,start_token,end_token) :\n",
        "            # (batch,vocab_size,embed_d)\n",
        "            print(\"============== Tokenization ===============\")\n",
        "            x = self.batch_tokenization(x,start_token,end_token)\n",
        "            x = self.embedding(x)\n",
        "            pos = self.positional_embedding(x)\n",
        "            x = self.dropout(x+pos)\n",
        "            return x\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "MdGWRvy5N2aC"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "      def __init__(self,epsilon=1e-6):\n",
        "              super(LayerNorm,self).__init__()\n",
        "              self.epsilon = epsilon\n",
        "\n",
        "      def forward(self,x) :\n",
        "              print(\"============== LayerNormalization ===============\", x.size())\n",
        "              batch_size,seq_length,model_d = x.shape\n",
        "              gamma = nn.Parameter(torch.ones(model_d).to(device))\n",
        "              beta = nn.Parameter(torch.zeros(model_d).to(device))\n",
        "              mean = x.mean(-1,keepdim=True)\n",
        "              var = x.var(-1,keepdim=True)\n",
        "              x_normalized = (x - mean) / torch.sqrt(var + self.epsilon)\n",
        "\n",
        "              x = gamma * x_normalized + beta\n",
        "              return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "P7P7dwHirIiu"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module) :\n",
        "      def __init__(self,input_d,model_d,heads_num) :\n",
        "                  super(MultiHeadAttention,self).__init__()\n",
        "                  self.input_d = input_d\n",
        "                  self.model_d = model_d\n",
        "                  self.heads_num = heads_num\n",
        "                  self.qkv_d = model_d // heads_num\n",
        "                  self.queryP = nn.Linear(model_d,model_d)\n",
        "                  self.valueP = nn.Linear(model_d,model_d)\n",
        "                  self.keyP = nn.Linear(model_d,model_d)\n",
        "                  self.out = nn.Linear(model_d,model_d)\n",
        "      def attention(self,q,k,v,mask=None) :\n",
        "             dk = torch.tensor(k.shape[-1],dtype=torch.float32)\n",
        "             energy = torch.matmul(q,k.transpose(-2,-1)) / torch.sqrt(dk)\n",
        "             if False :\n",
        "                        mask = mask.unsqueeze(1).unsqueeze(3)\n",
        "                        energy =  energy.masked_fill(mask != 0, float('-1e9'))\n",
        "             return torch.matmul(torch.softmax(energy,dim=-1),v)\n",
        "      def forward(self,x,mask=None):\n",
        "              \"\"\"\n",
        "               first we create key,query and value using a linear projection using a 1 fully connected layer\n",
        "               The size of these tensors is (input_sequence_length,model_d)\n",
        "              \"\"\"\n",
        "              print(\"============== MultiHeadAttention ===============\")\n",
        "              query= None\n",
        "              key = None\n",
        "              value = None\n",
        "              batch_size = None\n",
        "              if isinstance(x,(list,tuple)) :\n",
        "                   query,key,value = x\n",
        "                   batch_size = query.size(0)\n",
        "                   query = self.queryP(query)\n",
        "                   key = self.keyP(key)\n",
        "                   value = self.valueP(value)\n",
        "\n",
        "              else :\n",
        "                   batch_size = x.size(0)\n",
        "                   query = self.queryP(x)\n",
        "                   key = self.keyP(x)\n",
        "                   value = self.valueP(x)\n",
        "\n",
        "              \"\"\"\n",
        "               we add another dimension for heads now the tensors size is (heads_num,input_sequence,model_d)\n",
        "               calculte attention for each head independently and in parallel\n",
        "              \"\"\"\n",
        "              query = query.view(batch_size,self.heads_num,self.input_d,self.qkv_d)\n",
        "              key = key.view(batch_size, self.heads_num,self.input_d, self.qkv_d)\n",
        "              value = value.view(batch_size, self.heads_num,self.input_d, self.qkv_d)\n",
        "              attention = self.attention(query, key, value, mask)\n",
        "              attention = attention.view(batch_size, self.input_d, self.model_d)\n",
        "              out = self.out(attention)\n",
        "              return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "NZvinSs45GIM"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "class FeedForward(nn.Module) :\n",
        "        def __init__(self,input_size,output_size,hidden_size,dropout_p=0.1) :\n",
        "                  super(FeedForward,self).__init__()\n",
        "                  self.fc1 =  nn.Linear(input_size,hidden_size)\n",
        "                  self.fc2 = nn.Linear(hidden_size,output_size)\n",
        "                  self.dropout = nn.Dropout(p=dropout_p)\n",
        "        def forward(self,x) :\n",
        "                  print(\"============== FeedForward NN  ===============\", x.size())\n",
        "                  x = F.relu(self.fc1(x))\n",
        "                  x = self.dropout(x)\n",
        "                  x = self.fc2(x)\n",
        "                  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "Hvs851lvxa44"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module) :\n",
        "     def __init__(self,model_d,dropout_p,vocab_size,max_length,hidden_size=2048,attention_heads=8) :\n",
        "        super(EncoderLayer,self).__init__()\n",
        "        self.model_d = model_d\n",
        "        self.max_length = max_length\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.attention_heads = attention_heads\n",
        "        self.dropout1 = nn.Dropout(p=dropout_p)\n",
        "        self.dropout2 = nn.Dropout(p=dropout_p)\n",
        "        self.multi_head_attention =  MultiHeadAttention(self.max_length,self.model_d,self.attention_heads)\n",
        "        self.layernorm = LayerNorm()\n",
        "        self.fc = FeedForward(self.model_d,self.model_d,self.hidden_size)\n",
        "\n",
        "     def forward(self,x,mask=None) :\n",
        "              res_x = x.clone()\n",
        "              x = self.multi_head_attention(x,mask)\n",
        "              x = self.dropout1(x)\n",
        "              x = self.layernorm(x + res_x )\n",
        "              res_x = x.clone()\n",
        "              x = self.fc(x)\n",
        "              x = self.dropout2(x)\n",
        "              x =  self.layernorm(x + res_x )\n",
        "              return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "_AsFvF_AviXw"
      },
      "outputs": [],
      "source": [
        "class EncoderLayers(nn.Sequential) :\n",
        "\n",
        "          def forward(self, x,mask=None):\n",
        "                     for module in self._modules.values():\n",
        "                          x = module(x,mask)\n",
        "                     return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "jlLAww7VvrGg"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "           def __init__(self,model_d,dropout_p,num_layers,language_to_index,start_token,end_token,pad_token,vocab_size,max_length,hidden_size=2048,attention_heads=8) :\n",
        "                super(Encoder,self).__init__()\n",
        "                self.layers  = EncoderLayers(*[ EncoderLayer(model_d,dropout_p,vocab_size,max_length,hidden_size,attention_heads) for _ in range(num_layers)])\n",
        "                self.tokenizer = Tokenizer(model_d,max_sequence_length,language_to_index,start_token,end_token,pad_token)\n",
        "           def forward(self,x,mask,start_token,end_token) :\n",
        "                   x = self.tokenizer(x,start_token,end_token)\n",
        "                   x = self.layers(x,mask)\n",
        "                   return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "_f1sUiO_oKbe"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module) :\n",
        "        def __init__(self,model_d,dropout_p,vocab_size,max_length,hidden_size=2048,attention_heads=8) :\n",
        "            super(DecoderLayer,self).__init__()\n",
        "            self.model_d = model_d\n",
        "            self.max_length = max_length\n",
        "            self.hidden_size = hidden_size\n",
        "            self.vocab_size = vocab_size\n",
        "            self.attention_heads = attention_heads\n",
        "            self.dropout1 = nn.Dropout(p=dropout_p)\n",
        "            self.dropout2 = nn.Dropout(p=dropout_p)\n",
        "            self.dropout3 = nn.Dropout(p=dropout_p)\n",
        "            self.multi_head_attention =  MultiHeadAttention(self.max_length,self.model_d,self.attention_heads)\n",
        "            self.layernorm = LayerNorm()\n",
        "            self.fc = FeedForward(self.model_d,self.model_d,self.hidden_size)\n",
        "        def forward(self,x,encoder_out,att_mask,pad_mask) :\n",
        "                    res_x = x.clone()\n",
        "                    x = self.multi_head_attention(x,att_mask)\n",
        "                    x = self.dropout1(x)\n",
        "                    x = self.layernorm(x + res_x )\n",
        "                    res_x = x.clone()\n",
        "                    x  = self.multi_head_attention((x,encoder_out,encoder_out),pad_mask)\n",
        "                    x = self.dropout2(x)\n",
        "                    x = self.layernorm(x + res_x )\n",
        "                    res_x = x.clone()\n",
        "                    x =  self.fc(x)\n",
        "                    x = self.dropout3(x)\n",
        "                    x = self.layernorm(x + res_x )\n",
        "                    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "1AeZDQPxJw82"
      },
      "outputs": [],
      "source": [
        "class DecoderLayers(nn.Sequential) :\n",
        "\n",
        "          def forward(self, x, encoder_out,att_mask,pad_mask):\n",
        "                     for module in self._modules.values():\n",
        "                          x = module(x,encoder_out,att_mask,pad_mask)\n",
        "                     return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "q9s45PefNEm3"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "           def __init__(self,model_d,dropout_p,num_layers,language_to_index,start_token,end_token,pad_token,vocab_size,max_length,hidden_size=2048,attention_heads=8) :\n",
        "                super(Decoder,self).__init__()\n",
        "                self.layers  = DecoderLayers(*[ DecoderLayer(model_d,dropout_p,vocab_size,max_length,hidden_size,attention_heads) for _ in range(num_layers)])\n",
        "                self.tokenizer = Tokenizer(model_d,max_sequence_length,language_to_index,start_token,end_token,pad_token)\n",
        "\n",
        "           def forward(self,x,encoder_out,att_mask,pad_mask,start_token,end_token) :\n",
        "                   x = self.tokenizer(x,start_token,end_token)\n",
        "                   x = self.layers(x,encoder_out,att_mask,pad_mask)\n",
        "                   return  x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "oXwWQCtVxsRG"
      },
      "outputs": [],
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "batch_size = 30\n",
        "max_sequence_length = 200\n",
        "ffn_hidden = 2048\n",
        "num_layers = 5\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "MiyvpelcDf_F"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self,model_d,dropout_p,\n",
        "                num_layers,\n",
        "                english_to_index,\n",
        "                german_to_index,\n",
        "                start_token,\n",
        "                end_token,\n",
        "                pad_token\n",
        "                ,vocab_size,max_length,hidden_size=2048,attention_heads=8\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(model_d,dropout_p,num_layers,english_to_index,start_token,end_token,pad_token,vocab_size,max_length)\n",
        "        self.decoder = Decoder(model_d,dropout_p,num_layers,german_to_index,start_token,end_token,pad_token,vocab_size,max_length)\n",
        "        self.linear = nn.Linear(d_model, vocab_size)\n",
        "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    def forward(self,\n",
        "                x,\n",
        "                y,\n",
        "                encoder_pad_mask=None,\n",
        "                decoder_att_mask=None,\n",
        "                decoder_pad_mask=None,\n",
        "                enc_start_token=False,\n",
        "                enc_end_token=False,\n",
        "                dec_start_token=False, # We should make this true\n",
        "                dec_end_token=False): # x, y are batch of sentences\n",
        "        x = self.encoder(x,encoder_pad_mask ,start_token=enc_start_token, end_token=enc_end_token)\n",
        "        out = self.decoder(y,x,decoder_att_mask,decoder_pad_mask, start_token=dec_start_token, end_token=dec_end_token)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "B9AsmS8rwL1l"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "file_path = \"./deu.txt\"\n",
        "start_token = '<start>'\n",
        "end_token = '<end>'\n",
        "pad_token = '<pad>'\n",
        "english_vocabulary = [start_token,'<unk>', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/','`','’',\n",
        "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                      ':', '<', '=', '>', '?', '@','[', '\\\\', ']', '^', '_', '`',\n",
        "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
        "                        'y', 'z',\n",
        "                        '{', '|', '}', '~', pad_token, end_token]\n",
        "\n",
        "german_vocabulary = [start_token,'<unk>',' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/','`','’',\n",
        "                      '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                      ':', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`',\n",
        "                      'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                      'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
        "                      'y', 'z', 'ä', 'ö', 'ü', 'ß',\n",
        "                      '{', '|', '}', '~', pad_token, end_token]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "8CB0ftrQYek2"
      },
      "outputs": [],
      "source": [
        "index_to_german = {k : v  for k,v in enumerate(german_vocabulary)}\n",
        "german_to_index = {v : k  for k,v in enumerate(german_vocabulary)}\n",
        "index_to_english = {k : v  for k,v in enumerate(english_vocabulary)}\n",
        "english_to_index = {v : k  for k,v in enumerate(english_vocabulary)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "AqYqQdcOZCzL"
      },
      "outputs": [],
      "source": [
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    raw_data = file.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "c5HA1J1laXQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6615daf-a762-45a2-ccfe-54f90edd8983"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('go.', 'geh.'),\n",
              " ('hi.', 'hallo!'),\n",
              " ('hi.', 'grüß gott!'),\n",
              " ('run!', 'lauf!'),\n",
              " ('run.', 'lauf!'),\n",
              " ('wow!', 'potzdonner!'),\n",
              " ('wow!', 'donnerwetter!'),\n",
              " ('fire!', 'feuer!'),\n",
              " ('help!', 'hilfe!'),\n",
              " ('help!', 'zu hülf!'),\n",
              " ('stop!', 'stopp!'),\n",
              " ('wait!', 'warte!'),\n",
              " ('wait.', 'warte.'),\n",
              " ('begin.', 'fang an.'),\n",
              " ('go on.', 'mach weiter.'),\n",
              " ('hello!', 'hallo!'),\n",
              " ('hurry!', 'beeil dich!'),\n",
              " ('hurry!', 'schnell!'),\n",
              " ('i hid.', 'ich versteckte mich.'),\n",
              " ('i hid.', 'ich habe mich versteckt.'),\n",
              " ('i ran.', 'ich rannte.'),\n",
              " ('i see.', 'ich verstehe.'),\n",
              " ('i see.', 'aha.'),\n",
              " ('i try.', 'ich probiere es.'),\n",
              " ('i won!', 'ich hab gewonnen!'),\n",
              " ('i won!', 'ich habe gewonnen!'),\n",
              " ('relax.', 'entspann dich.'),\n",
              " ('shoot!', 'feuer!'),\n",
              " ('shoot!', 'schieß!'),\n",
              " ('smile.', 'lächeln!'),\n",
              " ('ask me.', 'frag mich!'),\n",
              " ('ask me.', 'fragt mich!'),\n",
              " ('ask me.', 'fragen sie mich!'),\n",
              " ('attack!', 'angriff!'),\n",
              " ('attack!', 'attacke!'),\n",
              " ('cheers!', 'zum wohl!'),\n",
              " ('eat it.', 'iss es.'),\n",
              " ('eat up.', 'iss auf.'),\n",
              " ('eat up.', 'iss auf!'),\n",
              " ('freeze!', 'keine bewegung!'),\n",
              " ('freeze!', 'stehenbleiben!'),\n",
              " ('go now.', 'geh jetzt!'),\n",
              " ('got it!', 'verstanden!'),\n",
              " ('got it!', 'aha!'),\n",
              " ('got it!', 'ich habs!'),\n",
              " ('got it?', 'kapiert?'),\n",
              " ('got it?', 'verstanden?'),\n",
              " ('got it?', 'einverstanden?'),\n",
              " ('he ran.', 'er rannte.'),\n",
              " ('he ran.', 'er lief.'),\n",
              " ('hop in.', 'mach mit!'),\n",
              " ('hop in.', 'spring rein!'),\n",
              " ('hug me.', 'drück mich!'),\n",
              " ('hug me.', 'nimm mich in den arm!'),\n",
              " ('hug me.', 'umarme mich!'),\n",
              " ('i care.', 'mir ist es wichtig.'),\n",
              " ('i fell.', 'ich fiel.'),\n",
              " ('i fell.', 'ich fiel hin.'),\n",
              " ('i fell.', 'ich stürzte.'),\n",
              " ('i fell.', 'ich bin hingefallen.'),\n",
              " ('i fell.', 'ich bin gestürzt.'),\n",
              " ('i fled.', 'ich flüchtete.'),\n",
              " ('i fled.', 'ich bin geflüchtet.'),\n",
              " ('i know.', 'ich weiß.'),\n",
              " ('i lied.', 'ich habe gelogen.'),\n",
              " ('i lost.', 'ich habe verloren.'),\n",
              " ('i paid.', 'ich habe bezahlt.'),\n",
              " ('i paid.', 'ich zahlte.'),\n",
              " ('i sang.', 'ich sang.'),\n",
              " ('i spit.', 'ich spuckte.'),\n",
              " ('i spit.', 'ich habe gespuckt.'),\n",
              " ('i swim.', 'ich schwimme.'),\n",
              " ('i wept.', 'ich weinte.'),\n",
              " ('i wept.', 'ich habe geweint.'),\n",
              " (\"i'm 19.\", 'ich bin 19 jahre alt.'),\n",
              " (\"i'm 19.\", 'ich bin 19.'),\n",
              " (\"i'm ok.\", \"mir geht's gut.\"),\n",
              " (\"i'm ok.\", 'es geht mir gut.'),\n",
              " (\"i'm up.\", 'ich bin wach.'),\n",
              " (\"i'm up.\", 'ich bin auf.'),\n",
              " ('no way!', 'unmöglich!'),\n",
              " ('no way!', 'das kommt nicht in frage!'),\n",
              " ('no way!', 'das gibt’s doch nicht!'),\n",
              " ('no way!', 'ausgeschlossen!'),\n",
              " ('no way!', 'in keinster weise!'),\n",
              " ('really?', 'wirklich?'),\n",
              " ('really?', 'echt?'),\n",
              " ('really?', 'im ernst?'),\n",
              " ('thanks.', 'danke!'),\n",
              " ('try it.', 'versuch’s!'),\n",
              " ('we try.', 'wir versuchen es.'),\n",
              " ('we won.', 'wir haben gewonnen.'),\n",
              " ('why me?', 'warum ich?'),\n",
              " ('ask tom.', 'frag tom!'),\n",
              " ('ask tom.', 'fragen sie tom!'),\n",
              " ('ask tom.', 'fragt tom!'),\n",
              " ('awesome!', 'fantastisch!'),\n",
              " ('be cool.', 'entspann dich!'),\n",
              " ('be fair.', 'sei nicht ungerecht!'),\n",
              " ('be fair.', 'sei fair!'),\n",
              " ('be kind.', 'sei nett!'),\n",
              " ('be nice.', 'sei nett!'),\n",
              " ('be nice.', 'seien sie nett!'),\n",
              " ('beat it.', 'geh weg!'),\n",
              " ('beat it.', 'hau ab!'),\n",
              " ('beat it.', 'verschwinde!'),\n",
              " ('beat it.', 'verdufte!'),\n",
              " ('beat it.', 'mach dich fort!'),\n",
              " ('beat it.', 'zieh leine!'),\n",
              " ('beat it.', 'mach dich vom acker!'),\n",
              " ('beat it.', 'verzieh dich!'),\n",
              " ('beat it.', 'verkrümele dich!'),\n",
              " ('beat it.', 'troll dich!'),\n",
              " ('beat it.', 'zisch ab!'),\n",
              " ('beat it.', 'pack dich!'),\n",
              " ('beat it.', 'mach ’ne fliege!'),\n",
              " ('beat it.', 'schwirr ab!'),\n",
              " ('beat it.', 'mach die sause!'),\n",
              " ('beat it.', 'scher dich weg!'),\n",
              " ('beat it.', 'scher dich fort!'),\n",
              " ('burn it.', 'verbrenne es!'),\n",
              " ('burn it.', 'verbrennt es!'),\n",
              " ('burn it.', 'verbrennen sie es!'),\n",
              " ('bury it.', 'vergrabe es!'),\n",
              " ('call me.', 'ruf mich an.'),\n",
              " ('come in.', 'komm herein.'),\n",
              " ('come in.', 'herein!'),\n",
              " ('come on!', 'komm!'),\n",
              " ('come on!', 'kommt!'),\n",
              " ('come on!', 'mach schon!'),\n",
              " ('come on!', 'macht schon!'),\n",
              " ('come on.', 'komm schon!'),\n",
              " ('drop it.', 'fallen lassen!'),\n",
              " ('fold it.', 'falte es zusammen!'),\n",
              " ('get tom.', 'hol tom.'),\n",
              " ('get out!', 'raus!'),\n",
              " ('get out!', 'geht raus!'),\n",
              " ('get out.', 'geh raus.'),\n",
              " ('get out.', 'geht raus!'),\n",
              " ('go away!', 'geh weg!'),\n",
              " ('go away!', 'hau ab!'),\n",
              " ('go away!', 'verschwinde!'),\n",
              " ('go away!', 'verdufte!'),\n",
              " ('go away!', 'mach dich fort!'),\n",
              " ('go away!', 'zieh leine!'),\n",
              " ('go away!', 'mach dich vom acker!'),\n",
              " ('go away!', 'verzieh dich!'),\n",
              " ('go away!', 'verkrümele dich!'),\n",
              " ('go away!', 'troll dich!'),\n",
              " ('go away!', 'zisch ab!'),\n",
              " ('go away!', 'pack dich!'),\n",
              " ('go away!', 'mach ’ne fliege!'),\n",
              " ('go away!', 'schwirr ab!'),\n",
              " ('go away!', 'mach die sause!'),\n",
              " ('go away!', 'scher dich weg!'),\n",
              " ('go away!', 'scher dich fort!'),\n",
              " ('go away.', 'geh weg!'),\n",
              " ('go away.', 'verpiss dich!'),\n",
              " ('go away.', 'hau ab!'),\n",
              " ('go away.', 'verschwinde!'),\n",
              " ('go away.', 'verdufte!'),\n",
              " ('go away.', 'mach dich fort!'),\n",
              " ('go away.', 'zieh leine!'),\n",
              " ('go away.', 'mach dich vom acker!'),\n",
              " ('go away.', 'verzieh dich!'),\n",
              " ('go away.', 'verkrümele dich!'),\n",
              " ('go away.', 'troll dich!'),\n",
              " ('go away.', 'zisch ab!'),\n",
              " ('go away.', 'pack dich!'),\n",
              " ('go away.', 'mach ’ne fliege!'),\n",
              " ('go away.', 'schwirr ab!'),\n",
              " ('go away.', 'mach die sause!'),\n",
              " ('go away.', 'scher dich weg!'),\n",
              " ('go away.', 'scher dich fort!'),\n",
              " ('go away.', 'gehen sie weg.'),\n",
              " ('go home.', 'geh nach hause.'),\n",
              " ('go home.', 'geh heim.'),\n",
              " ('goodbye!', 'auf wiedersehen!'),\n",
              " ('goodbye!', 'leb wohl!'),\n",
              " ('goodbye!', 'tschüss!'),\n",
              " ('hang on!', 'nicht nachlassen!'),\n",
              " ('hang on.', 'warte!'),\n",
              " ('he came.', 'er kam.'),\n",
              " ('he quit.', 'er hat gekündigt.'),\n",
              " ('he runs.', 'er rennt.'),\n",
              " ('he runs.', 'er läuft.'),\n",
              " ('help me.', 'hilf mir.'),\n",
              " ('help us.', 'hilf uns!'),\n",
              " ('help us.', 'helft uns!'),\n",
              " ('help us.', 'helfen sie uns!'),\n",
              " ('hi, tom.', 'hallo, tom!'),\n",
              " ('hit tom.', 'schlage tom!'),\n",
              " ('hit tom.', 'schlagt tom!'),\n",
              " ('hit tom.', 'schlagen sie tom!'),\n",
              " ('hold it!', 'nicht bewegen!'),\n",
              " ('hold it!', 'warte!'),\n",
              " ('hold it.', 'stehen bleiben!'),\n",
              " ('hold it.', 'bleib stehen!'),\n",
              " ('hold it.', 'bleiben sie stehen!'),\n",
              " ('hold it.', 'bleibt stehen!'),\n",
              " ('hold on.', 'warten sie kurz!'),\n",
              " ('hug tom.', 'umarme tom!'),\n",
              " ('hug tom.', 'umarmen sie tom!'),\n",
              " ('hug tom.', 'umarmt tom!'),\n",
              " ('hug tom.', 'drückt tom!'),\n",
              " ('hug tom.', 'drücken sie tom!'),\n",
              " ('hug tom.', 'drück tom!'),\n",
              " ('i agree.', 'ich bin einverstanden.'),\n",
              " ('i cried.', 'ich weinte.'),\n",
              " ('i cried.', 'ich habe geweint.'),\n",
              " ('i drove.', 'ich fuhr!'),\n",
              " ('i drove.', 'ich bin gefahren!'),\n",
              " ('i froze.', 'ich fror.'),\n",
              " ('i froze.', 'ich habe gefroren.'),\n",
              " ('i snore.', 'ich schnarche.'),\n",
              " ('i swore.', 'ich habe geschworen.'),\n",
              " ('i swore.', 'ich schwur.'),\n",
              " ('i waved.', 'ich winkte.'),\n",
              " ('i waved.', 'ich habe gewunken.'),\n",
              " (\"i'll go.\", 'ich gehe.'),\n",
              " (\"i'm bad.\", 'ich bin schlecht.'),\n",
              " (\"i'm fat.\", 'ich bin fett.'),\n",
              " (\"i'm fat.\", 'ich bin dick.'),\n",
              " (\"i'm hit!\", 'ich wurde getroffen!'),\n",
              " (\"i'm mad.\", 'ich bin verrückt.'),\n",
              " (\"i'm new.\", 'ich bin neu.'),\n",
              " (\"i'm old.\", 'ich bin alt.'),\n",
              " (\"i'm sad.\", 'ich bin traurig.'),\n",
              " (\"i'm shy.\", 'ich bin schüchtern.'),\n",
              " (\"i'm wet.\", 'ich bin nass.'),\n",
              " (\"it's ok.\", 'es ist in ordnung.'),\n",
              " (\"it's me!\", 'ich bin’s.'),\n",
              " (\"it's me.\", 'ich bin’s.'),\n",
              " ('join us.', 'komm mit.'),\n",
              " ('join us.', 'kommt mit.'),\n",
              " ('keep it.', 'behalt es!'),\n",
              " ('keep it.', 'behalt ihn!'),\n",
              " ('keep it.', 'behalt sie!'),\n",
              " ('keep it.', 'behalten sie ihn!'),\n",
              " ('keep it.', 'behalten sie sie!'),\n",
              " ('keep it.', 'behalten sie es!'),\n",
              " ('keep it.', 'behaltet ihn!'),\n",
              " ('keep it.', 'behaltet sie!'),\n",
              " ('keep it.', 'behaltet es!'),\n",
              " ('kill it.', 'töte es!'),\n",
              " ('kiss me.', 'küsst mich.'),\n",
              " ('lie low.', 'leg dich hin!'),\n",
              " ('lie low.', 'hinlegen!'),\n",
              " ('lie low.', 'niederlegen!'),\n",
              " ('lie low.', 'leg dich nieder!'),\n",
              " ('lie low.', 'legt euch nieder!'),\n",
              " ('lie low.', 'legen sie sich nieder!'),\n",
              " ('lie low.', 'legt euch hin!'),\n",
              " ('lie low.', 'legen sie sich hin!'),\n",
              " ('lock it.', 'schließ es ab.'),\n",
              " ('lock it.', 'schließe sie ab.'),\n",
              " ('lock it.', 'schließ ihn ab.'),\n",
              " ('open it.', 'öffne es!'),\n",
              " ('open it.', 'öffnet es!'),\n",
              " ('open it.', 'öffnen sie es!'),\n",
              " ('open up.', 'mach auf!'),\n",
              " ('perfect!', 'perfekt!'),\n",
              " ('pull it.', 'zieh dran.'),\n",
              " ('push it.', 'drück drauf.'),\n",
              " ('see you!', 'tschüss!'),\n",
              " ('see you.', 'wir sehen uns.'),\n",
              " ('show me.', \"zeig's mir!\"),\n",
              " ('shut up!', 'halt’s maul!'),\n",
              " ('so long.', 'bis später!'),\n",
              " ('stop it.', 'hör auf.'),\n",
              " ('take it.', 'nimm es.'),\n",
              " ('take it.', 'nehmt es.'),\n",
              " ('take it.', 'nehmen sie es.'),\n",
              " ('tell me.', 'sag es mir.'),\n",
              " ('tell me.', \"erzähl's mir.\"),\n",
              " ('tom ate.', 'tom aß.'),\n",
              " ('tom ate.', 'tom hat gegessen.'),\n",
              " ('tom hid.', 'thomas versteckte sich.'),\n",
              " ('tom hid.', 'thomas hat sich versteckt.'),\n",
              " ('tom ran.', 'tom rannte.'),\n",
              " ('tom ran.', 'tom ist gerannt.'),\n",
              " ('tom won.', 'tom hat gewonnen.'),\n",
              " ('wait up.', 'warte mal!'),\n",
              " ('wait up.', 'warten sie mal!'),\n",
              " ('wait up.', 'wartet mal!'),\n",
              " ('wake up!', 'wach auf!'),\n",
              " ('wake up!', 'wachen sie auf!'),\n",
              " ('wake up.', 'wach auf!'),\n",
              " ('wake up.', 'wachen sie auf!'),\n",
              " ('wash up.', 'wasch dir die hände.'),\n",
              " ('wash up.', 'wasch dir das gesicht.'),\n",
              " ('we lost.', 'wir haben verloren.'),\n",
              " ('welcome.', 'willkommen!'),\n",
              " ('who ate?', 'wer hat gegessen?'),\n",
              " ('who ate?', 'wer aß?'),\n",
              " ('who ran?', 'wer rannte?'),\n",
              " ('who ran?', 'wer ist gerannt?'),\n",
              " ('who won?', 'wer hat gewonnen?'),\n",
              " ('you run.', 'du läufst.'),\n",
              " ('you run.', 'sie laufen.'),\n",
              " ('you won.', 'du hast gewonnen.'),\n",
              " ('all rise.', 'alle aufstehen!'),\n",
              " ('am i fat?', 'bin ich dick?'),\n",
              " ('ask them.', 'frag sie.'),\n",
              " ('back off.', 'komm nicht näher!'),\n",
              " ('be a man.', 'sei ein mann!'),\n",
              " ('be brave.', 'sei tapfer!'),\n",
              " ('be brave.', 'seien sie tapfer!'),\n",
              " ('be brave.', 'seid tapfer!'),\n",
              " ('be brief.', 'fassen sie sich kurz.'),\n",
              " ('be brief.', 'fass dich kurz.'),\n",
              " ('be brief.', 'fasst euch kurz.'),\n",
              " ('be still.', 'sei ruhig.'),\n",
              " ('call tom.', 'ruf tom an!'),\n",
              " ('call tom.', 'rufe tom an!'),\n",
              " ('call tom.', 'rufen sie tom an!'),\n",
              " ('call tom.', 'ruft tom an!'),\n",
              " ('can i go?', 'kann ich gehen?'),\n",
              " ('catch me.', 'fang mich!'),\n",
              " ('catch me.', 'fangen sie mich!'),\n",
              " ('catch me.', 'fangt mich!'),\n",
              " ('cheer up!', 'kopf hoch!'),\n",
              " ('cool off!', 'reg dich ab!'),\n",
              " ('cuff him.', 'leg ihm handschellen an.'),\n",
              " ('cuff him.', 'legen sie ihm handschellen an.'),\n",
              " (\"don't go.\", 'geh nicht.'),\n",
              " ('drive on.', 'fahr weiter.'),\n",
              " ('find tom.', 'finde tom.'),\n",
              " ('find tom.', 'findet tom.'),\n",
              " ('find tom.', 'finden sie tom.'),\n",
              " ('fix this.', 'beheben sie das.'),\n",
              " ('fix this.', 'behebe das.'),\n",
              " ('fix this.', 'repariere das.'),\n",
              " ('fix this.', 'reparieren sie das.'),\n",
              " ('get away!', 'geh weg!'),\n",
              " ('get away!', 'verpiss dich!'),\n",
              " ('get away!', 'hau ab!'),\n",
              " ('get away!', 'verschwinde!'),\n",
              " ('get away!', 'verdufte!'),\n",
              " ('get away!', 'mach dich fort!'),\n",
              " ('get away!', 'zieh leine!'),\n",
              " ('get away!', 'mach dich vom acker!'),\n",
              " ('get away!', 'verzieh dich!'),\n",
              " ('get away!', 'verkrümele dich!'),\n",
              " ('get away!', 'troll dich!'),\n",
              " ('get away!', 'zisch ab!'),\n",
              " ('get away!', 'pack dich!'),\n",
              " ('get away!', 'mach ’ne fliege!'),\n",
              " ('get away!', 'schwirr ab!'),\n",
              " ('get away!', 'mach die sause!'),\n",
              " ('get away!', 'scher dich weg!'),\n",
              " ('get away!', 'scher dich fort!'),\n",
              " ('get down!', 'runter!'),\n",
              " ('get down!', 'hinlegen!'),\n",
              " ('get down!', 'in deckung!'),\n",
              " ('get down.', 'komm runter.'),\n",
              " ('get down.', 'kommen sie runter.'),\n",
              " ('get lost!', 'geh weg!'),\n",
              " ('get lost!', 'verpiss dich!'),\n",
              " ('get lost!', 'hau ab!'),\n",
              " ('get lost!', 'verschwinde!'),\n",
              " ('get lost!', 'verdufte!'),\n",
              " ('get lost!', 'mach dich fort!'),\n",
              " ('get lost!', 'zieh leine!'),\n",
              " ('get lost!', 'mach dich vom acker!'),\n",
              " ('get lost!', 'verzieh dich!'),\n",
              " ('get lost!', 'verkrümele dich!'),\n",
              " ('get lost!', 'troll dich!'),\n",
              " ('get lost!', 'zisch ab!'),\n",
              " ('get lost!', 'pack dich!'),\n",
              " ('get lost!', 'mach ’ne fliege!'),\n",
              " ('get lost!', 'schwirr ab!'),\n",
              " ('get lost!', 'mach die sause!'),\n",
              " ('get lost!', 'scher dich weg!'),\n",
              " ('get lost!', 'scher dich fort!'),\n",
              " ('get real!', 'jetzt mal ernsthaft!'),\n",
              " ('go ahead!', 'nur zu!'),\n",
              " ('go ahead.', 'mach weiter!'),\n",
              " ('grab tom.', 'hol tom.'),\n",
              " ('grab him.', 'greif ihn dir!'),\n",
              " ('have fun.', 'viel vergnügen!'),\n",
              " ('have fun.', 'viel spaß!'),\n",
              " ('have fun.', 'feier schön!'),\n",
              " ('he spoke.', 'er sprach.'),\n",
              " ('he tried.', 'er versuchte es.'),\n",
              " ('he tried.', 'er hat es versucht.'),\n",
              " ('he tries.', 'er versucht es.'),\n",
              " ('he tries.', 'er versuchte es.'),\n",
              " ('help tom.', 'hilf tom!'),\n",
              " ('help tom.', 'helft tom!'),\n",
              " ('help tom.', 'helfen sie tom!'),\n",
              " ('how cute!', 'was ist das nicht süß!'),\n",
              " ('how cute!', 'wie süß!'),\n",
              " ('how deep?', 'wie tief?'),\n",
              " ('how nice!', 'wie schön!'),\n",
              " ('how rude!', 'wie unverschämt!'),\n",
              " ('how rude!', 'wie unhöflich!'),\n",
              " ('humor me.', 'tu mir den gefallen.'),\n",
              " ('humor me.', 'tun sie mir den gefallen.'),\n",
              " ('hurry up.', 'beeil dich!'),\n",
              " ('hurry up.', 'beeil dich.'),\n",
              " ('hurry up.', 'mach hin!'),\n",
              " ('i can go.', 'ich kann gehen.'),\n",
              " ('i cursed.', 'ich fluchte.'),\n",
              " ('i cursed.', 'ich habe geflucht.'),\n",
              " ('i did it.', 'ich habe es geschafft.'),\n",
              " ('i did it.', \"ich hab's gemacht.\"),\n",
              " ('i get by.', 'ich komme klar.'),\n",
              " ('i get by.', 'ich komme zurecht.'),\n",
              " ('i get by.', 'ich komme über die runden.'),\n",
              " ('i get it.', 'ich verstehe.'),\n",
              " ('i got it.', 'ich habe es verstanden.'),\n",
              " ('i got it.', 'ich habe es bekommen.'),\n",
              " ('i got it.', \"ich hab's.\"),\n",
              " ('i helped.', 'ich half.'),\n",
              " ('i helped.', 'ich habe geholfen.'),\n",
              " ('i jumped.', 'ich bin gesprungen.'),\n",
              " ('i moaned.', 'ich stöhnte.'),\n",
              " ('i moaned.', 'ich ächzte.'),\n",
              " ('i nodded.', 'ich nickte.'),\n",
              " ('i paused.', 'ich hielt an.'),\n",
              " ('i paused.', 'ich pausierte.'),\n",
              " ('i prayed.', 'ich betete.'),\n",
              " ('i refuse.', 'ich weigere mich.'),\n",
              " ('i resign.', 'ich trete zurück.'),\n",
              " ('i rested.', 'ich rastete.'),\n",
              " ('i shaved.', 'ich rasierte mich.'),\n",
              " ('i shaved.', 'ich habe mich rasiert.'),\n",
              " ('i smiled.', 'ich lächelte.'),\n",
              " ('i stayed.', 'ich blieb.'),\n",
              " ('i stayed.', 'ich bin dageblieben.'),\n",
              " ('i use it.', 'ich benutze es.'),\n",
              " ('i waited.', 'ich habe gewartet.'),\n",
              " ('i winked.', 'ich blinzelte.'),\n",
              " ('i winked.', 'ich zwinkerte.'),\n",
              " ('i yawned.', 'ich gähnte.'),\n",
              " ('i yawned.', 'ich habe gegähnt.'),\n",
              " (\"i'll pay.\", 'ich werde zahlen.'),\n",
              " (\"i'll try.\", 'ich werde es versuchen.'),\n",
              " (\"i'm a dj.\", 'ich bin dj.'),\n",
              " (\"i'm back.\", 'ich bin wieder da.'),\n",
              " (\"i'm bald.\", 'ich bin glatzköpfig.'),\n",
              " (\"i'm bald.\", 'ich habe eine glatze.'),\n",
              " (\"i'm busy.\", 'ich bin beschäftigt.'),\n",
              " (\"i'm busy.\", 'ich habe zu tun.'),\n",
              " (\"i'm cool.\", 'ich bin cool.'),\n",
              " (\"i'm deaf.\", 'ich bin taub.'),\n",
              " (\"i'm deaf.\", 'ich bin gehörlos.'),\n",
              " (\"i'm fair.\", 'ich bin gerecht.'),\n",
              " (\"i'm fine.\", \"mir geht's gut.\"),\n",
              " (\"i'm fine.\", 'mir geht es gut.'),\n",
              " (\"i'm fine.\", 'es geht mir gut.'),\n",
              " (\"i'm free.\", 'ich bin frei.'),\n",
              " (\"i'm full.\", 'ich bin satt.'),\n",
              " (\"i'm game.\", 'ich bin dabei.'),\n",
              " (\"i'm game.\", 'ich mache mit.'),\n",
              " (\"i'm good.\", \"mir geht's gut.\"),\n",
              " (\"i'm here.\", 'ich bin hier.'),\n",
              " (\"i'm home.\", 'ich bin zu hause.'),\n",
              " (\"i'm late.\", 'ich komme zu spät.'),\n",
              " (\"i'm lost.\", 'ich habe mich verirrt.'),\n",
              " (\"i'm mean.\", 'ich bin gemein.'),\n",
              " (\"i'm next.\", 'ich bin der nächste.'),\n",
              " (\"i'm okay.\", 'es geht mir gut.'),\n",
              " (\"i'm rich.\", 'ich bin reich.'),\n",
              " (\"i'm safe.\", 'ich bin sicher.'),\n",
              " (\"i'm sick.\", 'ich bin krank!'),\n",
              " (\"i'm sure.\", 'ich bin sicher.'),\n",
              " (\"i'm tall.\", 'ich bin groß.'),\n",
              " (\"i'm thin.\", 'ich bin dünn.'),\n",
              " (\"i'm tidy.\", 'ich bin ordentlich.'),\n",
              " (\"i'm ugly.\", 'ich bin hässlich.'),\n",
              " (\"i'm weak.\", 'ich bin schwach.'),\n",
              " (\"i'm well.\", 'mir geht es gut.'),\n",
              " ('it helps.', 'das hilft.'),\n",
              " ('it hurts.', 'es tut weh.'),\n",
              " ('it hurts.', 'es schmerzt.'),\n",
              " ('it works.', 'es klappt.'),\n",
              " ('it works.', 'es funktioniert.'),\n",
              " (\"it's tom.\", 'es ist tom.'),\n",
              " (\"it's dry.\", 'es ist trocken.'),\n",
              " (\"it's his.\", 'es ist seins.'),\n",
              " (\"it's hot.\", 'es ist heiß.'),\n",
              " (\"it's hot.\", 'sie ist heiß.'),\n",
              " (\"it's hot.\", 'er ist heiß.'),\n",
              " (\"it's new.\", 'es ist neu.'),\n",
              " (\"it's sad.\", 'es ist traurig.'),\n",
              " ('keep out!', 'eintritt verboten!'),\n",
              " ('keep out.', 'komm nicht herein.'),\n",
              " ('keep out.', 'kein zutritt.'),\n",
              " ('kill tom.', 'töte thomas!'),\n",
              " ('kill tom.', 'tötet thomas!'),\n",
              " ('kill tom.', 'töten sie thomas!'),\n",
              " ('kiss tom.', 'küsse tom!'),\n",
              " ('kiss tom.', 'küssen sie tom!'),\n",
              " ('kiss tom.', 'küsst tom!'),\n",
              " ('leave us.', 'geh weg!'),\n",
              " ('leave us.', 'gehen sie weg.'),\n",
              " ('leave us.', 'lass uns allein.'),\n",
              " ('leave us.', 'lassen sie uns allein.'),\n",
              " (\"let's go!\", 'lass uns gehen!'),\n",
              " (\"let's go!\", \"auf geht's!\"),\n",
              " (\"let's go!\", 'gehen wir!'),\n",
              " (\"let's go!\", 'lasst uns gehen.'),\n",
              " (\"let's go!\", 'lass uns losgehen.'),\n",
              " (\"let's go!\", 'lasst uns losgehen.'),\n",
              " (\"let's go!\", 'auf, auf!'),\n",
              " ('look out!', 'vorsicht!'),\n",
              " ('marry me.', 'heirate mich.'),\n",
              " ('may i go?', 'darf ich gehen?'),\n",
              " ('said who?', 'wer sagt das?'),\n",
              " ('said who?', 'sagt wer?'),\n",
              " ('save tom.', 'rette tom!'),\n",
              " ('save tom.', 'rettet tom!'),\n",
              " ('save tom.', 'retten sie tom!'),\n",
              " ('she came.', 'sie kam.'),\n",
              " ('she lied.', 'sie log.'),\n",
              " ('she runs.', 'sie rennt.'),\n",
              " ('sit down!', 'setz dich!'),\n",
              " ('sit down!', 'setzen sie sich!'),\n",
              " ('sit down.', 'setz dich!'),\n",
              " ('speak up!', 'sprich lauter!'),\n",
              " ('stand up!', 'stehen sie auf!'),\n",
              " ('stand up!', 'steht auf!'),\n",
              " ('stand up!', 'stehe auf!'),\n",
              " ('stand up.', 'steh auf.'),\n",
              " ('stop tom.', 'halte tom auf!'),\n",
              " ('stop tom.', 'halten sie tom auf!'),\n",
              " ('stop tom.', 'haltet tom auf!'),\n",
              " ('take tom.', 'nimm tom.'),\n",
              " ('taste it.', 'probier es mal.'),\n",
              " ('tell tom.', 'sag es tom.'),\n",
              " ('tell tom.', 'sagen sie es tom.'),\n",
              " ('tell tom.', 'erzähl es tom.'),\n",
              " ('tell tom.', 'erzählen sie tom davon.'),\n",
              " ('tell tom.', 'sagt es tom.'),\n",
              " ('terrific!', 'hervorragend!'),\n",
              " ('terrific!', 'sagenhaft!'),\n",
              " ('terrific!', 'wunderbar!'),\n",
              " ('they won.', 'sie haben gewonnen.'),\n",
              " ('tom came.', 'tom kam.'),\n",
              " ('tom came.', 'tom ist gekommen.'),\n",
              " ('tom died.', 'tom ist gestorben.'),\n",
              " ('tom died.', 'tom starb.'),\n",
              " ('tom fell.', 'tom fiel.'),\n",
              " ('tom fell.', 'tom ist gefallen.'),\n",
              " ('tom knew.', 'tom wusste es.'),\n",
              " ('tom knew.', 'tom hat es gewusst.'),\n",
              " ('tom knew.', 'tom wusste bescheid.'),\n",
              " ('tom left.', 'tom ist gegangen.'),\n",
              " ('tom left.', 'tom ging.'),\n",
              " ('tom lied.', 'tom log.'),\n",
              " ('tom lies.', 'tom lügt.'),\n",
              " ('tom lost.', 'tom hat verloren.'),\n",
              " ('tom paid.', 'tom hat gezahlt.'),\n",
              " ('tom quit.', 'tom hat aufgehört.'),\n",
              " ('tom sang.', 'tom sang.'),\n",
              " ('tom spit.', 'thomas spuckte.'),\n",
              " ('tom spit.', 'thomas hat gespuckt.'),\n",
              " ('tom swam.', 'tom schwamm.'),\n",
              " ('tom swam.', 'tom ist geschwommen.'),\n",
              " ('tom went.', 'tom ging.'),\n",
              " ('tom wept.', 'tom weinte.'),\n",
              " (\"tom's up.\", 'tom ist auf.'),\n",
              " ('too late.', 'zu spät.'),\n",
              " ('touch it.', 'fass es an.'),\n",
              " ('trust me.', 'vertraue mir.'),\n",
              " ('trust me.', 'vertraut mir!'),\n",
              " ('trust me.', 'vertrauen sie mir!'),\n",
              " ('try hard.', 'versuch es richtig!'),\n",
              " ('try some.', 'probier mal.'),\n",
              " ('try this.', 'probier mal!'),\n",
              " ('use this.', 'nimm das hier!'),\n",
              " ('warn tom.', 'warnen sie tom.'),\n",
              " ('warn tom.', 'warne tom.'),\n",
              " ('watch me.', 'schau mir zu.'),\n",
              " ('watch me.', 'schauen sie mir zu.'),\n",
              " ('watch us.', 'beobachte uns.'),\n",
              " ('watch us.', 'beobachten sie uns.'),\n",
              " ('watch us.', 'schau uns zu.'),\n",
              " ('watch us.', 'schaut uns zu.'),\n",
              " ('watch us.', 'schauen sie uns zu.'),\n",
              " ('watch us.', 'beobachtet uns.'),\n",
              " ('we agree.', 'wir stimmen zu.'),\n",
              " ('we agree.', 'wir sind einverstanden.'),\n",
              " ('we tried.', 'wir haben es versucht.'),\n",
              " ('we tried.', 'wir versuchten es.'),\n",
              " (\"we'll go.\", 'wir werden gehen.'),\n",
              " (\"we're ok.\", 'wir sind in ordnung.'),\n",
              " ('what for?', 'wozu?'),\n",
              " ('what fun!', 'was für ein spaß!'),\n",
              " ('who am i?', 'wer bin ich?'),\n",
              " ('who came?', 'wer ist gekommen?'),\n",
              " ('who came?', 'wer kam?'),\n",
              " ('who died?', 'wer ist gestorben?'),\n",
              " ('who fell?', 'wer fiel?'),\n",
              " ('who fell?', 'wer ist gefallen?'),\n",
              " ('who lost?', 'wer hat verloren?'),\n",
              " ('who quit?', 'wer hat aufgehört?'),\n",
              " ('who quit?', 'wer ist ausgeschieden?'),\n",
              " ('who swam?', 'wer schwamm?'),\n",
              " ('who swam?', 'wer ist geschwommen?'),\n",
              " (\"who's he?\", 'wer ist er?'),\n",
              " ('write me.', 'schreib mir!'),\n",
              " ('write me.', 'schreiben sie mir!'),\n",
              " ('write me.', 'schreibt mir!'),\n",
              " ('you lost.', 'sie haben verloren.'),\n",
              " ('you lost.', 'du hast verloren.'),\n",
              " ('you lost.', 'ihr habt verloren.'),\n",
              " ('aim. fire!', 'zielen. feuer!'),\n",
              " ('am i dead?', 'bin ich tot?'),\n",
              " ('am i late?', 'bin ich zu spät?'),\n",
              " ('answer me.', 'antworten sie mir.'),\n",
              " ('be seated.', 'setz dich!'),\n",
              " ('be seated.', 'setzen sie sich.'),\n",
              " ('birds fly.', 'vögel fliegen.'),\n",
              " ('bless you.', 'gesundheit.'),\n",
              " ('call home!', 'ruf zuhause an!'),\n",
              " ('call home!', 'rufen sie zuhause an!'),\n",
              " ('calm down!', 'beruhige dich!'),\n",
              " ('calm down.', 'beruhige dich!'),\n",
              " ('calm down.', 'beruhigen sie sich!'),\n",
              " ('calm down.', 'beruhigen sie sich.'),\n",
              " ('can i eat?', 'kann ich essen?'),\n",
              " ('can we go?', 'können wir gehen?'),\n",
              " ('catch tom.', 'fang tom!'),\n",
              " ('catch tom.', 'fange tom!'),\n",
              " ('catch tom.', 'fangen sie tom!'),\n",
              " ('catch tom.', 'fangt tom!'),\n",
              " ('chill out.', 'entspann dich.'),\n",
              " ('come back.', 'komm wieder!'),\n",
              " ('come here.', 'komm hierher.'),\n",
              " ('come here.', 'komm her!'),\n",
              " ('come home.', 'kommt heim!'),\n",
              " ('come home.', 'komm heim!'),\n",
              " ('come over!', 'komm hierher!'),\n",
              " ('come over.', 'komm her!'),\n",
              " ('come soon.', 'komm bald.'),\n",
              " ('come soon.', 'kommen sie bald.'),\n",
              " ('cool down.', 'beruhige dich!'),\n",
              " ('did i win?', 'habe ich gewonnen?'),\n",
              " ('do it now.', 'mach es jetzt!'),\n",
              " ('dogs bark.', 'hunde bellen.'),\n",
              " (\"don't ask.\", 'frag nicht.'),\n",
              " (\"don't cry.\", 'weine nicht!'),\n",
              " (\"don't cry.\", 'weint nicht.'),\n",
              " (\"don't cry.\", 'weinen sie nicht.'),\n",
              " (\"don't cry.\", 'weinen sie nicht!'),\n",
              " (\"don't cry.\", 'weint nicht!'),\n",
              " (\"don't die.\", 'stirb nicht!'),\n",
              " (\"don't lie.\", 'lüge nicht!'),\n",
              " (\"don't lie.\", 'lüge nicht.'),\n",
              " (\"don't run.\", 'lauf nicht.'),\n",
              " ('excuse me.', 'es tut mir leid.'),\n",
              " ('excuse me.', 'entschuldigung!'),\n",
              " ('excuse me.', 'entschuldigung.'),\n",
              " ('excuse me.', 'entschuldigen sie!'),\n",
              " ('fantastic!', 'fantastisch!'),\n",
              " ('fantastic!', 'ganz toll!'),\n",
              " ('feel this.', 'fühl mal.'),\n",
              " ('follow me.', 'folge mir.'),\n",
              " ('forget it!', 'vergiss es!'),\n",
              " ('forget it!', 'daraus wird nichts.'),\n",
              " ('forget it.', 'vergiss es.'),\n",
              " ('forget it.', 'das kannst du knicken.'),\n",
              " ('forget me.', 'vergiss mich!'),\n",
              " ('forget me.', 'vergesst mich!'),\n",
              " ('forget me.', 'vergessen sie mich!'),\n",
              " ('get ready.', 'mach dich bereit!'),\n",
              " ('get ready.', 'mach dich fertig.'),\n",
              " ('go for it.', 'nur zu!'),\n",
              " ('go for it.', 'tue es.'),\n",
              " ('go get it.', \"hol's dir!\"),\n",
              " ('go inside.', 'geh rein!'),\n",
              " ('go inside.', 'komm rein.'),\n",
              " ('go to bed.', 'geh schlafen!'),\n",
              " ('go to bed.', 'geh ins bett!'),\n",
              " ('go to bed.', 'geht ins bett!'),\n",
              " ('go to bed.', 'legt euch schlafen!'),\n",
              " ('goodnight.', 'gute nacht.'),\n",
              " ('goodnight.', 'schönen abend.'),\n",
              " ('grab that.', 'nimm das.'),\n",
              " ('grab that.', 'hol das.'),\n",
              " ('hands off.', 'hände weg!'),\n",
              " ('have some.', 'nimm dir davon.'),\n",
              " ('have some.', 'nehmen sie davon.'),\n",
              " ('he is ill.', 'er ist krank.'),\n",
              " ('he is old.', 'er ist alt.'),\n",
              " ('he shaved.', 'er rasierte sich.'),\n",
              " ('he smiled.', 'er lächelte.'),\n",
              " (\"he's a dj.\", 'er ist dj.'),\n",
              " (\"he's a dj.\", 'er ist plattenaufleger.'),\n",
              " (\"he's fast.\", 'er ist schnell.'),\n",
              " (\"he's good.\", 'er ist gut.'),\n",
              " (\"he's lazy.\", 'er ist träge.'),\n",
              " (\"he's rich.\", 'er ist reich.'),\n",
              " (\"he's sexy.\", 'er ist sexy.'),\n",
              " ('head east.', 'fahr richtung osten.'),\n",
              " ('head east.', 'fahren sie richtung osten.'),\n",
              " ('head east.', 'fahrt richtung osten.'),\n",
              " ('here i am.', 'hier bin ich.'),\n",
              " (\"here's $5.\", 'hier sind fünf dollar.'),\n",
              " ('hold fire.', 'nicht schießen!'),\n",
              " ('hold this.', 'halt das!'),\n",
              " ('hold this.', 'halt das mal.'),\n",
              " ('hold this.', 'halten sie das!'),\n",
              " ('hold this.', 'haltet das!'),\n",
              " ('how awful!', 'schrecklich!'),\n",
              " ('how is it?', 'wie ist die lage?'),\n",
              " ('how weird!', 'wie seltsam!'),\n",
              " ('humor tom.', 'lass tom doch seinen willen.'),\n",
              " ('humor tom.', 'lasst tom doch seinen willen.'),\n",
              " ('i am lost.', 'ich habe mich verirrt.'),\n",
              " ('i am okay.', 'es geht mir gut.'),\n",
              " ('i am sure.', 'ich bin sicher.'),\n",
              " ('i am tall.', 'ich bin groß.'),\n",
              " ('i am well.', 'mir geht es gut.'),\n",
              " ('i am well.', 'es geht mir gut.'),\n",
              " ('i approve.', 'ich bin einverstanden.'),\n",
              " ('i can fly.', 'ich kann fliegen.'),\n",
              " ('i can run.', 'ich kann laufen.'),\n",
              " ('i can run.', 'ich kann rennen.'),\n",
              " ('i can ski.', 'ich kann ski fahren.'),\n",
              " ('i can win.', 'ich kann gewinnen.'),\n",
              " ('i changed.', 'ich änderte.'),\n",
              " ('i cheated.', 'ich habe geschummelt.'),\n",
              " ('i clapped.', 'ich klatschte.'),\n",
              " ('i coughed.', 'ich hustete.'),\n",
              " ('i coughed.', 'ich habe gehustet.'),\n",
              " ('i escaped.', 'ich entkam.'),\n",
              " ('i escaped.', 'ich bin entkommen.'),\n",
              " ('i fainted.', 'ich wurde ohnmächtig.'),\n",
              " ('i fainted.', 'ich fiel in ohnmacht.'),\n",
              " ('i fear so.', 'ich fürchte, ja.'),\n",
              " ('i frowned.', 'ich runzelte die stirn.'),\n",
              " ('i gave up.', 'ich habe aufgegeben.'),\n",
              " ('i gave up.', 'ich gab auf.'),\n",
              " ('i get you.', 'ich verstehe, was du meinst.'),\n",
              " ('i giggled.', 'ich kicherte.'),\n",
              " ('i give in.', 'ich gebe nach.'),\n",
              " ('i give in.', 'ich füge mich.'),\n",
              " ('i got mad.', 'ich wurde wütend.'),\n",
              " ('i got mad.', 'ich wurde böse.'),\n",
              " ('i got you.', 'ich habe dich.'),\n",
              " ('i had fun.', 'ich hatte spaß.'),\n",
              " ('i had fun.', 'ich habe mich amüsiert.'),\n",
              " ('i hit tom.', 'ich habe tom geschlagen.'),\n",
              " ('i hit tom.', 'ich schlug tom.'),\n",
              " ('i hope so.', 'das hoffe ich.'),\n",
              " ('i hope so.', 'ich hoffe es.'),\n",
              " ('i hung up.', 'ich habe aufgelegt.'),\n",
              " ('i hurried.', 'ich habe mich beeilt.'),\n",
              " ('i knew it.', 'ich wusste es.'),\n",
              " ('i knew it.', 'ich wusste das.'),\n",
              " ('i laughed.', 'ich lachte.'),\n",
              " ('i like it.', 'das gefällt mir.'),\n",
              " ('i like it.', 'es ist mir recht.'),\n",
              " ('i lost it.', 'ich habe ihn verloren.'),\n",
              " ('i love it!', 'ich liebe es!'),\n",
              " ('i love it.', 'ich liebe es.'),\n",
              " ('i made it.', 'ich habe es gemacht.'),\n",
              " ('i made it.', 'ich habe es geschafft.'),\n",
              " ('i may win.', 'vielleicht gewinne ich.'),\n",
              " ('i mean it!', 'ich meine es so!'),\n",
              " ('i mean it!', 'ich meine es ernst!'),\n",
              " ('i mean it.', 'ich meine es ernst.'),\n",
              " ('i mean it.', 'es ist ernst gemeint von mir.'),\n",
              " ('i met tom.', 'ich habe tom getroffen.'),\n",
              " ('i met tom.', 'ich traf tom.'),\n",
              " ('i met tom.', 'ich bin tom begegnet.'),\n",
              " ('i met him.', 'ich habe ihn getroffen.'),\n",
              " ('i miss it.', 'ich vermisse es.'),\n",
              " ('i miss it.', 'es fehlt mir.'),\n",
              " ('i promise.', \"ich versprech's.\"),\n",
              " ('i promise.', 'ich verspreche es.'),\n",
              " ('i relaxed.', 'ich entspanne mich.'),\n",
              " ('i relaxed.', 'ich habe mich entspannt.'),\n",
              " ('i retired.', 'ich ging in pension.'),\n",
              " ('i said no.', 'ich sagte nein.'),\n",
              " ('i saw tom.', 'ich habe tom gesehen.'),\n",
              " ('i saw him.', 'ich habe ihn gesehen.'),\n",
              " ('i saw one.', 'ich habe einen gesehen.'),\n",
              " ('i saw you.', 'ich habe dich gesehen.'),\n",
              " ('i saw you.', 'ich habe euch gesehen.'),\n",
              " ('i saw you.', 'ich habe sie gesehen.'),\n",
              " ('i see tom.', 'ich sehe tom.'),\n",
              " ('i slipped.', 'ich rutschte aus.'),\n",
              " ('i slipped.', 'ich bin ausgerutscht.'),\n",
              " ('i stopped.', 'ich hörte auf.'),\n",
              " ('i stopped.', 'ich stoppte.'),\n",
              " ('i took it.', 'ich nahm es.'),\n",
              " ('i took it.', 'ich habe es genommen.'),\n",
              " ('i want it.', 'ich möchte es.'),\n",
              " ('i want it.', 'ich will es.'),\n",
              " ('i was new.', 'ich war neu.'),\n",
              " ('i was shy.', 'ich war schüchtern.'),\n",
              " ('i will go.', 'ich gehe.'),\n",
              " ('i will go.', 'ich werde gehen.'),\n",
              " ('i woke up.', 'ich wachte auf.'),\n",
              " (\"i'd do it.\", 'ich würde es tun.'),\n",
              " (\"i'd leave.\", 'ich würde gehen.'),\n",
              " (\"i'll call.\", 'ich werde anrufen.'),\n",
              " (\"i'll come.\", 'ich werde kommen.'),\n",
              " (\"i'll cook.\", 'ich werde kochen.'),\n",
              " (\"i'll help.\", 'ich helfe.'),\n",
              " (\"i'll live.\", 'ich werde leben.'),\n",
              " (\"i'll obey.\", 'ich werde gehorchen.'),\n",
              " (\"i'll quit.\", 'ich werde aufhören.'),\n",
              " (\"i'll quit.\", 'ich werde abbrechen.'),\n",
              " (\"i'll sing.\", 'ich werde singen.'),\n",
              " (\"i'll stay.\", 'ich werde bleiben.'),\n",
              " (\"i'll stop.\", 'ich werde aufhören.'),\n",
              " (\"i'll talk.\", 'ich werde reden.'),\n",
              " (\"i'll wait.\", 'ich werde warten.'),\n",
              " (\"i'll walk.\", 'ich gehe zu fuß.'),\n",
              " (\"i'll walk.\", 'ich werde zu fuß gehen.'),\n",
              " (\"i'm a man.\", 'ich bin ein mann.'),\n",
              " (\"i'm a spy.\", 'ich bin spion.'),\n",
              " (\"i'm a spy.\", 'ich bin spionin.'),\n",
              " (\"i'm angry.\", 'ich bin sauer.'),\n",
              " (\"i'm awake.\", 'ich bin wach.'),\n",
              " (\"i'm blind.\", 'ich bin blind.'),\n",
              " (\"i'm bored.\", 'ich bin gelangweilt.'),\n",
              " (\"i'm bored.\", 'mir ist langweilig.'),\n",
              " (\"i'm broke.\", 'ich bin knapp bei kasse.'),\n",
              " (\"i'm broke.\", 'ich bin pleite.'),\n",
              " (\"i'm crazy.\", 'ich bin verrückt.'),\n",
              " (\"i'm cured.\", 'ich bin geheilt.'),\n",
              " (\"i'm drunk.\", 'ich bin betrunken.'),\n",
              " (\"i'm drunk.\", 'ich bin blau.'),\n",
              " (\"i'm dying.\", 'ich werde bald sterben.'),\n",
              " (\"i'm early.\", 'ich bin früh dran.'),\n",
              " (\"i'm first.\", 'ich bin erster.'),\n",
              " (\"i'm first.\", 'ich bin erste.'),\n",
              " (\"i'm fussy.\", 'ich bin pingelig.'),\n",
              " (\"i'm going.\", 'ich gehe jetzt.'),\n",
              " (\"i'm happy.\", 'ich bin glücklich.'),\n",
              " (\"i'm happy.\", 'ich bin froh.'),\n",
              " (\"i'm lying.\", 'ich lüge.'),\n",
              " (\"i'm needy.\", 'ich bin bedürftig.'),\n",
              " (\"i'm obese.\", 'ich bin fett.'),\n",
              " (\"i'm picky.\", 'ich bin pingelig.'),\n",
              " (\"i'm picky.\", 'ich bin wählerisch.'),\n",
              " (\"i'm ready!\", 'ich bin soweit!'),\n",
              " (\"i'm ready.\", 'ich bin soweit.'),\n",
              " (\"i'm ready.\", 'ich bin so weit.'),\n",
              " (\"i'm right.\", 'ich habe recht.'),\n",
              " (\"i'm right.\", 'ich habe recht.'),\n",
              " (\"i'm sober.\", 'ich bin nüchtern.'),\n",
              " (\"i'm sorry.\", 'es tut mir leid.'),\n",
              " (\"i'm sorry.\", 'entschuldigung!'),\n",
              " (\"i'm stuck.\", 'ich stecke fest.'),\n",
              " (\"i'm tired.\", 'ich bin müde!'),\n",
              " (\"i'm tired.\", 'ich bin müde.'),\n",
              " (\"i'm tough.\", 'ich bin zäh.'),\n",
              " (\"i'm upset.\", 'ich bin verärgert.'),\n",
              " (\"i'm upset.\", 'ich bin bestürzt.'),\n",
              " (\"i'm yours.\", 'ich gehöre dir.'),\n",
              " (\"i've lost.\", 'ich habe verloren.'),\n",
              " ('ignore it.', 'beachte es gar nicht!'),\n",
              " ('ignore it.', 'beachten sie es gar nicht!'),\n",
              " ('is tom ok?', 'ist tom okay?'),\n",
              " ('is tom in?', 'ist tom da?'),\n",
              " ('is he tom?', 'ist er tom?'),\n",
              " ('is it bad?', 'ist es schlimm?'),\n",
              " ('is it far?', 'ist das weit?'),\n",
              " ('is it hot?', 'ist es heiß?'),\n",
              " ('is it new?', 'ist es neu?'),\n",
              " ('it burned.', 'es brannte.'),\n",
              " ('it burned.', 'es verbrannte.'),\n",
              " ('it poured.', 'es schüttete.'),\n",
              " ('it snowed.', 'es hat geschneit.'),\n",
              " ('it stinks.', 'da ist doch was faul.'),\n",
              " ('it worked.', 'es hat geklappt.'),\n",
              " ('it worked.', 'das hat funktioniert.'),\n",
              " (\"it's 3:30.\", 'es ist halb vier.'),\n",
              " (\"it's 7:45.\", 'es ist 7:45 uhr.'),\n",
              " (\"it's 7:45.\", 'es ist sieben uhr fünfundvierzig.'),\n",
              " (\"it's 8:30.\", 'es ist 8:30 uhr.'),\n",
              " (\"it's 9:15.\", 'es ist viertel nach neun.'),\n",
              " (\"it's 9:15.\", 'es ist 9.15\\xa0uhr.'),\n",
              " (\"it's a tv.\", 'das ist ein fernseher.'),\n",
              " (\"it's cold.\", 'es ist kalt.'),\n",
              " (\"it's cool.\", 'es ist kühl.'),\n",
              " (\"it's dark.\", 'es ist dunkel.'),\n",
              " (\"it's easy.\", 'das ist einfach.'),\n",
              " (\"it's easy.\", 'es ist einfach.'),\n",
              " (\"it's fair.\", 'es ist gerecht.'),\n",
              " (\"it's free.\", 'es ist frei.'),\n",
              " (\"it's hard.\", 'das ist hart.'),\n",
              " (\"it's here.\", 'es ist hier.'),\n",
              " (\"it's late.\", 'es ist spät.'),\n",
              " (\"it's mine.\", 'es ist meins.'),\n",
              " (\"it's mine.\", 'sie gehört mir.'),\n",
              " (\"it's mine.\", 'er gehört mir.'),\n",
              " (\"it's okay.\", 'es ist in ordnung.'),\n",
              " (\"it's open.\", 'es ist geöffnet.'),\n",
              " (\"it's ours.\", 'es ist unsers.'),\n",
              " (\"it's over.\", 'es ist vorbei.'),\n",
              " (\"it's sand.\", 'es ist sand.'),\n",
              " (\"it's true.\", 'es ist wahr.'),\n",
              " (\"it's work.\", 'es ist arbeit.'),\n",
              " (\"it's work.\", 'das ist meine arbeit.'),\n",
              " ('jump down.', 'spring runter!'),\n",
              " ('just swim.', 'schwimm einfach.'),\n",
              " ('keep away.', 'bleib weg.'),\n",
              " ('keep away.', 'bleibt weg.'),\n",
              " ('keep away.', 'bleiben sie weg.'),\n",
              " ('keep back.', 'bleiben sie zurück.'),\n",
              " ('keep calm.', 'bleib ruhig.'),\n",
              " ('keep calm.', 'bleiben sie ruhig.'),\n",
              " ('keep cool.', 'bleib gelassen!'),\n",
              " ('keep down.', 'bleib unten.'),\n",
              " ('keep that.', 'behalt’s!'),\n",
              " ('keep them.', 'behalte sie.'),\n",
              " ('keep them.', 'behalten sie sie.'),\n",
              " ('keep them.', 'behaltet sie.'),\n",
              " ('keep them.', 'behalt sie!'),\n",
              " ('keep this.', 'behalte das!'),\n",
              " ('keep warm.', 'halt dich warm.'),\n",
              " ('keep warm.', 'haltet euch warm.'),\n",
              " ('keep warm.', 'halten sie sich warm.'),\n",
              " ('kill them.', 'töte sie!'),\n",
              " ('kill them.', 'tötet sie!'),\n",
              " ('kill them.', 'töten sie sie!'),\n",
              " ('kill them.', 'bring sie um!'),\n",
              " ('kill them.', 'bringt sie um!'),\n",
              " ('kill them.', 'bringen sie sie um!'),\n",
              " ('leave tom.', 'verlasse tom.'),\n",
              " ('leave tom.', 'verlassen sie tom.'),\n",
              " ('leave now.', 'geh jetzt!'),\n",
              " ('leave now.', 'geht jetzt!'),\n",
              " ('let it be.', 'lass es sein.'),\n",
              " ('let it be.', 'lass es bleiben.'),\n",
              " ('let me be.', 'lass mich in ruhe.'),\n",
              " ('let me go!', 'lassen sie mich gehen!'),\n",
              " ('let me go!', 'lass mich gehen!'),\n",
              " ('let me in.', 'lass mich herein.'),\n",
              " ('let us go.', 'lass uns gehen.'),\n",
              " ('let us go.', 'lassen sie uns gehen.'),\n",
              " (\"let's eat.\", 'essen wir etwas!'),\n",
              " (\"let's see.\", 'schauen wir mal.'),\n",
              " (\"let's try!\", 'lasst es uns versuchen!'),\n",
              " (\"let's try!\", 'lass es uns versuchen!'),\n",
              " (\"let's try!\", 'lass es uns probieren!'),\n",
              " ('lie still.', 'lieg still und beweg dich nicht.'),\n",
              " ('listen up.', 'hör zu.'),\n",
              " ('look away.', 'schau weg.'),\n",
              " ('look back!', 'schau nach hinten!'),\n",
              " ('look back.', 'schau hinter dich.'),\n",
              " ('look down.', 'guck runter.'),\n",
              " ('look here.', 'schau hier.'),\n",
              " ('look here.', 'schau her.'),\n",
              " ('loosen it.', 'dreh es auf.'),\n",
              " ('loosen up.', 'mach dich locker!'),\n",
              " ('move over.', 'rutsch mal ein stück!'),\n",
              " ('nice shot!', 'guter schuss!'),\n",
              " ('of course!', 'natürlich!'),\n",
              " ('of course!', 'selbstverständlich!'),\n",
              " ('of course!', 'auf jeden fall!'),\n",
              " ('of course!', 'na klar!'),\n",
              " ('of course!', 'aber sicher doch!'),\n",
              " ('please go.', 'bitte geh.'),\n",
              " ('please go.', 'bitte geht.'),\n",
              " ('please go.', 'bitte gehen sie.'),\n",
              " ('put it on.', 'setz es auf.'),\n",
              " ('put it on.', 'tu es drauf.'),\n",
              " ('put it on.', 'setz ihn auf.'),\n",
              " ('put it on.', 'zieh sie an.'),\n",
              " ('put it on.', 'setz sie auf.'),\n",
              " ('put it on.', 'setz sie dir auf.'),\n",
              " ('put it on.', 'zieh sie dir an.'),\n",
              " ('put it on.', 'setz ihn dir auf.'),\n",
              " ('read this.', 'lies das hier.'),\n",
              " ('repeat it.', 'wiederhole es!'),\n",
              " ('repeat it.', 'wiederholen sie es'),\n",
              " ('repeat it.', 'wiederholt es!'),\n",
              " ('say \"aah.\"', 'sag „ah!“'),\n",
              " ('say hello.', 'sag hallo.'),\n",
              " ('see below.', 'siehe unten.'),\n",
              " ('seize him!', 'fass ihn!'),\n",
              " ('seize him!', 'fassen sie ihn!'),\n",
              " ('seize him!', 'fasst ihn!'),\n",
              " ('seriously?', 'wirklich?'),\n",
              " ('seriously?', 'echt?'),\n",
              " ('seriously?', 'ernsthaft?'),\n",
              " ('seriously?', 'im ernst?'),\n",
              " ('seriously?', 'echt jetzt?'),\n",
              " ('she cried.', 'sie weinte.'),\n",
              " ('she tried.', \"sie hat's versucht.\"),\n",
              " ('she walks.', 'sie geht.'),\n",
              " ('she walks.', 'sie geht zu fuß.'),\n",
              " (\"she's hot.\", 'sie ist heiß.'),\n",
              " ('sign here.', 'unterschreibe hier.'),\n",
              " ('sign here.', 'unterschreiben sie hier.'),\n",
              " ('sign this.', 'unterschreiben sie das.'),\n",
              " ('sign this.', 'unterschreib das.'),\n",
              " ('sit by me.', 'setz dich zu mir!'),\n",
              " ('sit still.', 'sitz still.'),\n",
              " ('sit still.', 'sitzen sie still.'),\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "sentences =  [ (sentence.rstrip(\"\\n\").split(\"\\t\")[0].lower(),sentence.rstrip(\"\\n\").split(\"\\t\")[1].lower()) for sentence in raw_data]\n",
        "sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "PGhVazwtg4wq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4bb6daa-1e2e-4874-c2c3-c96998380259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97th percentile length English: 61.0\n",
            "97th percentile length German: 73.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "PERCENTILE = 97\n",
        "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x[0]) for x in sentences], PERCENTILE)}\" )\n",
        "print( f\"{PERCENTILE}th percentile length German: {np.percentile([len(x[1]) for x in sentences], PERCENTILE)}\" )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "wtRGfigqhbU4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ca99611-9dc1-4005-c892-92ea4ef7e84d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "max_sequence_length = 80\n",
        "def is_token_exist(sentence,vocab):\n",
        "     for token in sentence :\n",
        "           if token not in vocab :\n",
        "                 return False\n",
        "     return True\n",
        "\n",
        "def  is_valid_length(sentence,max_sequence_length) :\n",
        "           return len(sentence) < max_sequence_length - 1\n",
        "\n",
        "is_token_exist('sie geht zu fuß.',german_vocabulary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "mOR1NJ31q6FL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "412a8c3d-63fb-4eb0-c0d2-cfdaf1843f80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences: 221533\n",
            "Number of valid sentences: 215827\n"
          ]
        }
      ],
      "source": [
        "valid_sentence_indicies = []\n",
        "for index in range(len(sentences)):\n",
        "    german_sentence, english_sentence = sentences[index][1], sentences[index][0]\n",
        "    if is_valid_length(german_sentence, max_sequence_length) \\\n",
        "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
        "      and is_token_exist(german_sentence, german_vocabulary):\n",
        "        valid_sentence_indicies.append(index)\n",
        "\n",
        "print(f\"Number of sentences: {len(sentences)}\")\n",
        "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "p-7BnSmxrxDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d6b1674-4075-489a-8b58-73e452dbff24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "73"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "d_model = 512\n",
        "batch_size = 30\n",
        "ffn_hidden = 2048\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "num_layers = 1\n",
        "max_sequence_length = 200\n",
        "german_vocab_size = len(german_vocabulary)\n",
        "transformer = Transformer(d_model,drop_prob,num_layers,english_to_index,german_to_index,start_token,end_token,pad_token,vocab_size=german_vocab_size,max_length=max_sequence_length)\n",
        "len(english_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "koGeNOjnwgqp"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader,Dataset\n",
        "class TextDataset(Dataset) :\n",
        "    def __init__(self,sentences) :\n",
        "         self.sentences = sentences\n",
        "\n",
        "\n",
        "    def __len__(self) :\n",
        "         return len(self.sentences)\n",
        "    def __getitem__(self,idx):\n",
        "           german = self.sentences[idx][1]\n",
        "           english = self.sentences[idx][0]\n",
        "           return english,german\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "7kRhTcYvzX50"
      },
      "outputs": [],
      "source": [
        "dataset = TextDataset(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "dB4PDDXSzj21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c7cb20c-61b2-42b6-8965-227a2f1518da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('run!', 'lauf!') 221533\n"
          ]
        }
      ],
      "source": [
        "print(dataset[3],len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "iMI3Q2p03f3M"
      },
      "outputs": [],
      "source": [
        "criterian = nn.CrossEntropyLoss(ignore_index=german_to_index[pad_token],reduction=\"none\")\n",
        "for params in transformer.parameters():\n",
        "    if params.dim() > 1:\n",
        "        nn.init.xavier_uniform_(params)\n",
        "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "Gs6eovqyiXco"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_masks(eng_batch, ger_batch):\n",
        "    num_sentences = len(eng_batch)\n",
        "    look_ahead_mask = torch.zeros((max_sequence_length, max_sequence_length))\n",
        "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
        "    encoder_padding_mask = torch.zeros((num_sentences, max_sequence_length, max_sequence_length))\n",
        "    decoder_padding_mask_self_attention = torch.zeros((num_sentences, max_sequence_length, max_sequence_length))\n",
        "    decoder_padding_mask_cross_attention = torch.zeros((num_sentences, max_sequence_length, max_sequence_length))\n",
        "\n",
        "    for idx in range(num_sentences):\n",
        "      eng_sentence_length, ger_sentence_length = len(eng_batch[idx]), len(ger_batch[idx])\n",
        "      eng_chars_to_padding_mask = np.arange(eng_sentence_length  , max_sequence_length)\n",
        "      ger_chars_to_padding_mask = np.arange(ger_sentence_length , max_sequence_length)\n",
        "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = 1\n",
        "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = 1\n",
        "      decoder_padding_mask_self_attention[idx, :, ger_chars_to_padding_mask] = 1\n",
        "      decoder_padding_mask_self_attention[idx, ger_chars_to_padding_mask, :] = 1\n",
        "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = 1\n",
        "      decoder_padding_mask_cross_attention[idx, ger_chars_to_padding_mask, :] = 1\n",
        "    decoder_self_attention_mask =   decoder_padding_mask_self_attention + look_ahead_mask\n",
        "    return encoder_padding_mask, decoder_self_attention_mask, decoder_padding_mask_cross_attention,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "PIhTI0llYaO4"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(dataset,batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r0oLQDX-eOe",
        "outputId": "99447228-3a8e-4610-8822-730f626ad3e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([30, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([30, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([30, 200, 512])\n"
          ]
        }
      ],
      "source": [
        "transformer.train()\n",
        "transformer.to(device)\n",
        "num_epochs = 15\n",
        "losses  = []\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    iterator = iter(train_loader)\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for batch_num, batch in enumerate(iterator):\n",
        "        transformer.train()\n",
        "        eng_batch, ger_batch = batch\n",
        "\n",
        "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, ger_batch)\n",
        "        optim.zero_grad()\n",
        "        ger_predictions = transformer(eng_batch,\n",
        "                                     ger_batch,\n",
        "                                     encoder_self_attention_mask.to(device),\n",
        "                                     decoder_self_attention_mask.to(device),\n",
        "                                     decoder_cross_attention_mask.to(device),\n",
        "                                     enc_start_token=False,\n",
        "                                     enc_end_token=False,\n",
        "                                     dec_start_token=True,\n",
        "                                     dec_end_token=True)\n",
        "        labels = transformer.decoder.tokenizer.batch_tokenization(ger_batch, start_token=False, end_token=True)\n",
        "        loss = criterian(\n",
        "            ger_predictions.view(-1, german_vocab_size).to(device),\n",
        "            labels.view(-1).to(device)\n",
        "        ).to(device)\n",
        "        valid_indicies = torch.where(labels.view(-1) == german_to_index[pad_token], False, True)\n",
        "        loss = loss.sum() / valid_indicies.sum()\n",
        "        epoch_loss+=loss.item()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "    losses.append(epoch_loss/len(iterator))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.title(\"Training loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "3-n-JdxiXp22",
        "outputId": "dc477d29-3787-4724-b81b-4f5b57ef6715"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.eval()\n",
        "def translate(eng_sentence) :\n",
        "     ger_sentence = (\"\",)\n",
        "     eng_sentence = (eng_sentence,)\n",
        "     with torch.no_grad():\n",
        "        for word_counter in range(max_sequence_length):\n",
        "                encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, ger_sentence)\n",
        "                predictions = transformer(eng_sentence,\n",
        "                                          ger_sentence,\n",
        "                                          encoder_self_attention_mask.to(device),\n",
        "                                          decoder_self_attention_mask.to(device),\n",
        "                                          decoder_cross_attention_mask.to(device),\n",
        "                                          enc_start_token=False,\n",
        "                                          enc_end_token=False,\n",
        "                                          dec_start_token=True,\n",
        "                                          dec_end_token=False)\n",
        "                next_token_prob_distribution = predictions[0][word_counter] # not actual probs\n",
        "                next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
        "                next_token = index_to_german[next_token_index]\n",
        "                print(\"next token\",next_token)\n",
        "                ger_sentence = (ger_sentence[0] + next_token, )\n",
        "                if next_token == end_token:\n",
        "                  break\n",
        "\n",
        "     print(f\"{eng_sentence} : {ger_sentence}\")\n"
      ],
      "metadata": {
        "id": "geeFzLhGSoeX"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"I love\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fITtBe5cuWe8",
        "outputId": "6db35540-9c06-4db9-f4f9-e3c7fffa36d4"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token r\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token a\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token l\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token t\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token u\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token z\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token u\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token k\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token u\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token k\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token k\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token u\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token k\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token u\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token f\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token r\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token r\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token a\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token a\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token i\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token .\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token g\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token g\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token s\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token r\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token s\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token i\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token i\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token r\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token s\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token s\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token i\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token n\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token  \n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token s\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== Tokenization ===============\n",
            "============ positional embedding ========== torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== MultiHeadAttention ===============\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "============== FeedForward NN  =============== torch.Size([1, 200, 512])\n",
            "============== LayerNormalization =============== torch.Size([1, 200, 512])\n",
            "next token e\n",
            "('I love',) : ('........................ .................................................. . . . . . . . . . .raltu... . . zu ku ken. ku kufen.ererenen an aneneneinen.enen gengeseren seineinerenen senenen en sein se',)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyM2OvNRASlss8m0d/wb+ZV6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}